{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1b0b99-8a70-4e69-b9e3-2bdf16d3cfa8",
   "metadata": {},
   "source": [
    "# Website Summarizer Notebook\n",
    "\n",
    "This notebook implements a website summarizer that uses Python, BeautifulSoup, Selenium (optional), and OpenAI's API to generate comprehensive summaries of webpages. \n",
    "\n",
    "Before running the notebook, please ensure that you have installed all required packages:\n",
    "\n",
    "- `requests`\n",
    "- `beautifulsoup4`\n",
    "- `python-dotenv`\n",
    "- `openai`\n",
    "- `selenium` (if you plan to fetch JavaScript-heavy pages)\n",
    "\n",
    "Also, create a `.env` file in the same directory with your OpenAI API key:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "Now, run the code cell below to start the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d7493-2653-44b9-b1f6-6e53e1a1593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ANSI color codes\n",
    "RED = \"\\033[1;31m\"\n",
    "GREEN = \"\\033[1;32m\"\n",
    "YELLOW = \"\\033[1;33m\"\n",
    "BLUE = \"\\033[1;34m\"\n",
    "MAGENTA = \"\\033[1;35m\"\n",
    "CYAN = \"\\033[1;36m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "# Define HTTP headers and (optionally) proxies\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "# Uncomment and set your proxies if needed:\n",
    "# PROXIES = {\"http\": \"http://your-proxy\", \"https\": \"http://your-proxy\"}\n",
    "PROXIES = None\n",
    "\n",
    "# File where the sites list is persisted\n",
    "SITES_FILE = \"sites.json\"\n",
    "\n",
    "\n",
    "def load_sites():\n",
    "    \"\"\"\n",
    "    Load sites from the SITES_FILE. If the file doesn't exist, create it with default sites.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(SITES_FILE):\n",
    "        default_sites = {\n",
    "            \"1\": {\"url\": \"https://www.cybersecurity-insiders.com/\", \"name\": \"Cybersecurity Insiders\"},\n",
    "            \"2\": {\"url\": \"https://www.darkreading.com/\", \"name\": \"Dark Reading\"},\n",
    "            \"3\": {\"url\": \"https://www.infosecurity-magazine.com/\", \"name\": \"Infosecurity Magazine\"},\n",
    "            \"4\": {\"url\": \"https://cnn.com/\", \"name\": \"CNN\"}\n",
    "        }\n",
    "        save_sites(default_sites)\n",
    "        return default_sites\n",
    "    else:\n",
    "        try:\n",
    "            with open(SITES_FILE, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading {SITES_FILE}: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "def save_sites(sites):\n",
    "    \"\"\"\n",
    "    Save the current sites dictionary to SITES_FILE.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(SITES_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sites, f, indent=4)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving sites to {SITES_FILE}: {e}\")\n",
    "\n",
    "\n",
    "def fetch_with_selenium(url):\n",
    "    \"\"\"\n",
    "    Use Selenium (with headless Chrome) to fetch pages that require JavaScript.\n",
    "    Make sure selenium and the appropriate WebDriver are installed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "    except ImportError:\n",
    "        logging.error(\"Selenium is not installed. Please install selenium to use JS-heavy page support.\")\n",
    "        return None\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing Selenium WebDriver: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching page with Selenium: {e}\")\n",
    "        html = None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return html\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Sanitize the filename by removing characters not allowed in file names.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
    "\n",
    "\n",
    "def is_navigation_link(a_tag):\n",
    "    \"\"\"\n",
    "    Return True if the <a> tag is inside a navigation element (nav, header, or footer).\n",
    "    \"\"\"\n",
    "    for parent in a_tag.parents:\n",
    "        if parent.name in ['nav', 'header', 'footer']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    Represents a webpage. Downloads and parses the page with BeautifulSoup,\n",
    "    extracts the title, text, and also saves all URL links that appear to be part of the main story.\n",
    "    If the initial request returns little content, attempts to use Selenium for JavaScript-heavy pages.\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        # Try to fetch using requests\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10, proxies=PROXIES)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error fetching the URL via requests: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'html' not in content_type:\n",
    "            logging.error(\"The URL did not return HTML content.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string.strip() if soup.title and soup.title.string else \"No Title\"\n",
    "        self.links = []\n",
    "        if soup.body:\n",
    "            # Extract <a> tags only if they are not inside nav, header, or footer elements.\n",
    "            for a in soup.body.find_all('a', href=True):\n",
    "                if is_navigation_link(a):\n",
    "                    continue\n",
    "                link_text = a.get_text().strip() or a['href']\n",
    "                link_url = a['href']\n",
    "                if not link_url.startswith(\"http\"):\n",
    "                    link_url = urljoin(self.url, link_url)\n",
    "                self.links.append((link_text, link_url))\n",
    "            # Now remove unwanted tags.\n",
    "            for tag in soup.body([\"script\", \"style\", \"img\", \"input\", \"nav\", \"footer\", \"header\"]):\n",
    "                tag.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "\n",
    "        # If the extracted text is very short, try Selenium as a fallback.\n",
    "        if not self.text.strip() or len(self.text) < 50:\n",
    "            logging.info(\"Page content appears minimal, trying Selenium for JavaScript-heavy content...\")\n",
    "            html = fetch_with_selenium(url)\n",
    "            if html:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                if soup.body:\n",
    "                    self.links = []\n",
    "                    for a in soup.body.find_all('a', href=True):\n",
    "                        if is_navigation_link(a):\n",
    "                            continue\n",
    "                        link_text = a.get_text().strip() or a['href']\n",
    "                        link_url = a['href']\n",
    "                        if not link_url.startswith(\"http\"):\n",
    "                            link_url = urljoin(self.url, link_url)\n",
    "                        self.links.append((link_text, link_url))\n",
    "                    for tag in soup.body([\"script\", \"style\", \"img\", \"input\", \"nav\", \"footer\", \"header\"]):\n",
    "                        tag.decompose()\n",
    "                    self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "                else:\n",
    "                    self.text = \"\"\n",
    "\n",
    "\n",
    "def user_prompt_for(website, max_chars=4000):\n",
    "    \"\"\"\n",
    "    Build a robust prompt for summarization including clear instructions.\n",
    "    If the text is too long, only the first max_chars characters are included.\n",
    "    \"\"\"\n",
    "    prompt = f\"You are analyzing the website titled '{website.title}'.\\n\"\n",
    "    prompt += (\n",
    "        \"Please provide a robust and comprehensive summary in markdown. \"\n",
    "        \"Include all key points, background details, and any news or announcements present. \"\n",
    "        \"Your summary should be complete and detailed enough to give a full understanding of the website's content.\\n\\n\"\n",
    "    )\n",
    "    if len(website.text) > max_chars:\n",
    "        prompt += website.text[:max_chars] + \"\\n\\n[Content truncated]\"\n",
    "    else:\n",
    "        prompt += website.text\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def messages_for(website, system_prompt):\n",
    "    \"\"\"\n",
    "    Build the list of messages in the format required by the OpenAI Chat Completion API.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n",
    "\n",
    "def summarize(url, model):\n",
    "    \"\"\"\n",
    "    Given a URL, scrape the website and get a robust summary from the OpenAI API.\n",
    "    Returns both the Website object and the summary.\n",
    "    \"\"\"\n",
    "    website = Website(url)\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that analyzes website content and provides a robust, complete, and comprehensive summary in markdown. \"\n",
    "        \"Include key details, context, and highlight any news or announcements.\"\n",
    "    )\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages_for(website, system_prompt)\n",
    "        )\n",
    "    except openai.error.OpenAIError as e:\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "        sys.exit(1)\n",
    "    summary = response.choices[0].message.content\n",
    "    return website, summary\n",
    "\n",
    "\n",
    "def format_summary_for_file(website, summary):\n",
    "    \"\"\"\n",
    "    Prepend the summary with a header that includes the title, current date, and attribution.\n",
    "    Then append a \"Links\" section in markdown format if any main-story links were extracted.\n",
    "    The header format is:\n",
    "        Title\n",
    "        Date: <today date>\n",
    "        By: GreyFriar\n",
    "    \n",
    "    Followed by a blank line, the summary content, and finally the links section.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    header = f\"{website.title}\\nDate: {current_date}\\nBy: GreyFriar\\n\\n\"\n",
    "    content = header + summary\n",
    "    if website.links:\n",
    "        content += \"\\n\\n## Links\\n\"\n",
    "        for text, url in website.links:\n",
    "            content += f\"- [{text}]({url})\\n\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_summary(website, summary, custom_name=None):\n",
    "    \"\"\"\n",
    "    Save the formatted summary to a local file.\n",
    "    The filename contains the current date and a sanitized version of either the website title or a custom name.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    content = format_summary_for_file(website, summary)\n",
    "    if custom_name:\n",
    "        filename = f\"{current_date}_{sanitize_filename(custom_name)}.md\"\n",
    "    else:\n",
    "        filename = f\"{current_date}_{sanitize_filename(website.title)}.md\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        logging.info(f\"Summary saved to file: {filename}\")\n",
    "        print(f\"{GREEN}Summary saved to file: {filename}{RESET}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving summary to file: {e}\")\n",
    "\n",
    "\n",
    "def choose_output_destination():\n",
    "    \"\"\"\n",
    "    Ask the user whether they want to see the result on screen, save to file, or both.\n",
    "    \"\"\"\n",
    "    print(f\"{YELLOW}Where do you want the result to go?{RESET}\")\n",
    "    print(f\"{GREEN}[1]{RESET} Screen\")\n",
    "    print(f\"{GREEN}[2]{RESET} File\")\n",
    "    print(f\"{GREEN}[3]{RESET} Both\")\n",
    "    option = input(f\"{BLUE}Enter your choice (1, 2, or 3): {RESET}\").strip()\n",
    "    return option\n",
    "\n",
    "\n",
    "def daily_summary(sites, model):\n",
    "    \"\"\"\n",
    "    Generate a combined daily summary from all persisted sites.\n",
    "    Each site's summary is prefixed with its header.\n",
    "    The final output is saved to a file and/or printed based on user choice.\n",
    "    \"\"\"\n",
    "    combined_summary = f\"# Daily Summary for {datetime.now().strftime('%Y-%m-%d')}\\n\\n\"\n",
    "    for key in sorted(sites, key=lambda x: int(x)):\n",
    "        site_info = sites[key]\n",
    "        url = site_info[\"url\"]\n",
    "        print(f\"{CYAN}Processing {url}... Please wait...{RESET}\")\n",
    "        website, summary = summarize(url, model)\n",
    "        header = f\"{website.title}\\nDate: {datetime.now().strftime('%Y-%m-%d')}\\nBy: GreyFriar\\n\\n\"\n",
    "        combined_summary += header + summary + \"\\n\\n---\\n\\n\"\n",
    "    filename = f\"{datetime.now().strftime('%Y-%m-%d')}_Daily_Summary.md\"\n",
    "    option = choose_output_destination()\n",
    "    if option in [\"2\", \"3\"]:\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(combined_summary)\n",
    "            logging.info(f\"Daily summary saved to file: {filename}\")\n",
    "            print(f\"{GREEN}Daily summary saved to file: {filename}{RESET}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving daily summary: {e}\")\n",
    "    if option in [\"1\", \"3\"]:\n",
    "        print(f\"\\n{YELLOW}Daily Summary:{RESET}\\n\")\n",
    "        print(combined_summary)\n",
    "\n",
    "\n",
    "def print_welcome_menu(sites):\n",
    "    \"\"\"\n",
    "    Print the welcome screen with ASCII art and the colored menu options.\n",
    "    \"\"\"\n",
    "    header = f\"\"\"{CYAN}\n",
    "     _____               ______    _            \n",
    "    |  __ \\              |  ___|  (_)           \n",
    "    | |  \\/_ __ ___ _   _| |_ _ __ _  __ _ _ __ \n",
    "    | | __| '__/ _ \\ | | |  _| '__| |/ _` | '__|\n",
    "    | |_\\ \\ | |  __/ |_| | | | |  | | (_| | |   \n",
    "     \\____/_|  \\___|\\__, \\_| |_|  |_|\\__,_|_|   \n",
    "                    __/ |                      \n",
    "                   |___/                       \n",
    "             Cyber Security News                    \n",
    "{RESET}\"\"\"\n",
    "    print(header)\n",
    "    print(f\"{YELLOW}Please choose an option:{RESET}\")\n",
    "    for key in sorted(sites, key=lambda x: int(x)):\n",
    "        site_info = sites[key]\n",
    "        print(f\"{GREEN}[{key}]{RESET} Summarize {site_info['url']} ({site_info['name']})\")\n",
    "    print(f\"{GREEN}[5]{RESET} Enter your own site URL\")\n",
    "    print(f\"{GREEN}[6]{RESET} Generate a daily summary from all default sites\")\n",
    "    print(f\"{GREEN}[7]{RESET} Add another site to the default sites list\")\n",
    "    print(f\"{GREEN}[8]{RESET} Remove a site from the default sites list\")\n",
    "    print(f\"{GREEN}[0]{RESET} End/Exit the program\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load environment variables from the .env file.\n",
    "    load_dotenv(override=True)\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key or not api_key.strip():\n",
    "        logging.error(\"No valid API key found. Please check your .env file.\")\n",
    "        sys.exit(1)\n",
    "    openai.api_key = api_key\n",
    "    logging.info(\"API key loaded successfully.\")\n",
    "\n",
    "    # Set the default model to use.\n",
    "    model = \"gpt-4o-mini\"  # Adjust as needed\n",
    "\n",
    "    # Load persisted sites from file.\n",
    "    sites = load_sites()\n",
    "\n",
    "    while True:\n",
    "        print_welcome_menu(sites)\n",
    "        choice = input(f\"{BLUE}Enter your choice: {RESET}\").strip().lower()\n",
    "        if choice in [\"0\", \"end\"]:\n",
    "            print(f\"{MAGENTA}Exiting. Goodbye!{RESET}\")\n",
    "            break\n",
    "        elif choice in sites:\n",
    "            site_info = sites[choice]\n",
    "            url = site_info[\"url\"]\n",
    "            print(f\"{CYAN}Processing {url}... Please wait...{RESET}\")\n",
    "            website, summary = summarize(url, model)\n",
    "            option = choose_output_destination()\n",
    "            if option in [\"1\", \"3\"]:\n",
    "                print(f\"\\n{YELLOW}Summary:{RESET}\\n\")\n",
    "                print(summary)\n",
    "            if option in [\"2\", \"3\"]:\n",
    "                save_summary(website, summary)\n",
    "        elif choice == \"5\":\n",
    "            url = input(f\"{BLUE}Enter the site URL: {RESET}\").strip()\n",
    "            if url:\n",
    "                print(f\"{CYAN}Processing {url}... Please wait...{RESET}\")\n",
    "                website, summary = summarize(url, model)\n",
    "                option = choose_output_destination()\n",
    "                if option in [\"1\", \"3\"]:\n",
    "                    print(f\"\\n{YELLOW}Summary:{RESET}\\n\")\n",
    "                    print(summary)\n",
    "                if option in [\"2\", \"3\"]:\n",
    "                    save_summary(website, summary)\n",
    "            else:\n",
    "                print(f\"{RED}No URL provided.{RESET}\")\n",
    "        elif choice == \"6\":\n",
    "            daily_summary(sites, model)\n",
    "        elif choice == \"7\":\n",
    "            new_url = input(f\"{BLUE}Enter the URL to add: {RESET}\").strip()\n",
    "            if not new_url:\n",
    "                print(f\"{RED}No URL provided. Returning to menu.{RESET}\")\n",
    "                continue\n",
    "            new_name = input(f\"{BLUE}Enter a name for this site (optional): {RESET}\").strip()\n",
    "            if not new_name:\n",
    "                new_name = new_url  # fallback if no name provided\n",
    "            next_key = str(max([int(k) for k in sites.keys()] + [0]) + 1)\n",
    "            sites[next_key] = {\"url\": new_url, \"name\": new_name}\n",
    "            save_sites(sites)\n",
    "            print(f\"{GREEN}Site added successfully as option [{next_key}].{RESET}\")\n",
    "        elif choice == \"8\":\n",
    "            # List current sites and ask which one to remove.\n",
    "            print(f\"{YELLOW}Current sites:{RESET}\")\n",
    "            for key in sorted(sites, key=lambda x: int(x)):\n",
    "                print(f\"{GREEN}[{key}]{RESET} {sites[key]['name']} ({sites[key]['url']})\")\n",
    "            rem_key = input(f\"{BLUE}Enter the option number to remove (or press Enter to cancel): {RESET}\").strip()\n",
    "            if rem_key in sites:\n",
    "                removed = sites.pop(rem_key)\n",
    "                save_sites(sites)\n",
    "                print(f\"{GREEN}Removed site: {removed['name']} ({removed['url']}).{RESET}\")\n",
    "            else:\n",
    "                print(f\"{RED}Invalid option. No site removed.{RESET}\")\n",
    "        else:\n",
    "            print(f\"{RED}Invalid choice. Please try again.{RESET}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
